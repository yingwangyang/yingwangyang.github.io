<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wangyang Ying</title>

    <meta name="author" content="Wangyang Ying">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wangyang Ying
                </p>
                <p>I'm a Ph.D. student at <a href="https://www.asu.edu">Arizona State Univeristy</a> in Tempe, advised by <a href="https://www.public.asu.edu/~yanjiefu/">Prof. Yanjie Fu</a>. 
                  I earned my Bachelor's and Master's degrees from <a href="https://www.scu.edu.cn">Sichuan University</a> in 2016 and 2019, respectively, under the supervision of <a href="https://machineilab.org/user/Lei_Zhang/index.htm">Prof. Lei Zhang</a>.
                  After my Master's, I worked at Alibaba and Tencent, focusing on video recommendation algorithms and News search algorithms, respectively. 
                  In 2023, I started my Ph.D. at ASU, and during my studies, I interned at <a href = "https://www.a-star.edu.sg/">ASTAR, Singapore</a>, advised by <a href="https://liangli-zhen.github.io/">Liangli Zhen</a>.
                </p>
                <p>
                  My research interests include data mining, machine learning, and interdisciplinary applications. Currently, I focus on Data-Centric AI, learning from unlabeled data, and AI for scientific discovery.
                </p>
                <p style="text-align:center">
                  <a href="mailto:yingwangyang@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=INs4SuoAAAAJ&hl=en">Scholar</a>
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://github.com/NanxuGong/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/wangyang.jpg"> <img style="width:100%;max-width:100%;height:auto;aspect-ratio: 1 / 1;object-fit: cover; object-position: center top; border-radius: 50%;" alt="profile photo" src="images/wangyang.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <p>
                  <b>[2025-03]</b> I will join NEC lab, Princeton as a research intern during the summer (Mayâ€“August 2025).
                </p>
                <p>
                  <b>[2024-12]</b> One paper has been accepted by AAAI 2025. 
                </p>
                <p>
                  <b>[2024-12]</b> One paper has been accepted by ACM TIST. 
                </p>
                <p>
                  <b>[2024-08]</b> One paper has been accepted by ACM TKDD. 
                </p>
                <p>
                  <b>[2024-07]</b> Two papers have been accepted by CIKM 2024. 
                </p>
                <p>
                  <b>[2024-05]</b> One paper has been accepted by KDD 2024. 
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Work Experience</h2>
              <p>
                <b>[05/2024~08/2024]</b> Research Intern, Institute of High Performance Computing, A*STAR, Singapore.
              </p>
              <p>
                <b>[11/2020~08/2022]</b> Full Time, Platform and Content Group, Tencent, Beijing.
              </p>
              <p>
                <b>[06/2019~10/2020]</b> Full Time, Digital Media & Entertainment Group, Alibaba, Beijing.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Service</h2>
            <p>
              <b>&#9679 ICLR</b> The International Conference on Learning Representations (2024).
            </p>
            <p>
              <b>&#9679 KDD</b> The ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2024, 2025).
            </p>
            <p>
              <b>&#9679 ICML</b> International Conference on Machine Learning (2025).
            </p>
            <p>
              <b>&#9679 CIKM</b> The Conference on Information and Knowledge Management (2023, 2024).
            </p>
            <p>
              <b>&#9679 TKDD</b> ACM Transactions on Knowledge Discovery from Data.
            </p>
            <p>
              <b>&#9679 TKDE</b> IEEE Transactions on Knowledge and Data Engineering.
            </p>
            <p>
              <b>&#9679 BigData</b> The IEEE International Conference on Big Data (2023, 2024).
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Teaching Experience</h2>
            <p>
              <b>[01/2025~05/2025]</b> Teaching Assistant for CSE 572 - Data Mining
            </p>
            <p>
              <b>[08/2024~12/2024]</b> Teaching Assistant for CSE 572 - Data Mining
            </p>
          </td>
        </tr>
      </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Selected Publications</h2> * Denote equal contribution
            </td>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/KDD2024.png' width=100%>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning</span>
                <br>
                <strong>Wangyang Ying</strong>, Dongjie Wang, Xuanming Hu, Yuanchun Zhou, Charu C. Aggarwal, Yanjie Fu
                <br>
                <em>KDD</em>, 2024 
                (<a href="https://dl.acm.org/doi/pdf/10.1145/3637528.3672015">Paper</a>)
                <p></p>
                <p>
                  We propose an unsupervised feature transformation learning (UFTL) framework that integrates graph contrastive learning and multi-objective fine-tuning to enhance feature set quality. 
                  It formulates feature transformation as a measurement-pretrain-finetune paradigm, 
                  where a feature set is represented as a feature-feature interaction graph, pre-trained using contrastive learning, 
                  and optimized via a deep generative model to construct more effective transformed features, particularly benefiting applications like material performance screening.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/TKDD2024.png' width=100%>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Feature Selection as Deep Sequential Generative Learning</span>
                <br>
                <strong>Wangyang Ying</strong>, Dongjie Wang, Haifeng Chen, Yanjie Fu
                <br>
                <em>TKDD</em>, 2024 
                (<a href="https://dl.acm.org/doi/pdf/10.1145/3687485">Paper</a>)
                <p></p>
                <p>
                  We propose a feature selection approach by framing it as a deep sequential generative learning task. 
                  The method utilizes a variational transformer-based encoder-decoder-evaluator framework to embed feature selection knowledge into a continuous space, 
                  enabling efficient gradient-based optimization and autoregressive generation of optimal feature subsets without relying on large discrete search spaces or extensive hyperparameter tuning.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/CIKM2024.png' width=100%>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Revolutionizing Biomarker Discovery:
                  Leveraging Generative AI for Bio-Knowledge-Embedded
                  Continuous Space Exploration</span>
                <br>
                <strong>Wangyan Ying</strong>, Dongjie Wang, Xuanming Hu, Ji Qiu, Jin Park, Yanjie Fu
                <br>
                <em>CIKM</em>, 2024(
                <a href="https://dl.acm.org/doi/pdf/10.1145/3627673.3680041">Paper</a>)
                <p></p>
                <p>
                  We introduce a generative AI-based framework for biomarker discovery by embedding biomarker identification knowledge into a continuous space. 
                  It employs an encoder-evaluator-decoder structure to optimize biomarker selection through gradient-based search and autoregressive sequence generation, 
                  improving efficiency and robustness in high-dimensional, low-sample-size biological datasets.
                </p>
              </td>
            </tr>

            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <img src='images/ICDM2023.png' width=100%>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <span class="papertitle">Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing</span>
              <br>
              <strong>Wangyang Ying</strong><sup>*</sup>, Dongjie Wang<sup>*</sup>, Kunpeng Liu, Leilei Sun, Yanjie Fu
              <br>
              <em>ICDM</em>, 2023 
              (<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415697">Paper</a>)
              <p></p>
              <p>
                We introduce a self-optimizing feature generation framework that integrates categorical hashing representation with hierarchical reinforcement feature crossing. 
                It optimizes feature generation by leveraging hashing-based discretization to enhance robustness against outliers and reinforcement learning to discover meaningful feature interactions, 
                efficiently navigating the search space for improved downstream predictive performance.
              </p>
            </td>
          </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/tist.png' width=100%>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Neuro-Symbolic Embedding for Short and Effective Feature Selection via
                    Autoregressive Generation</span>
                <br>
                Nanxu Gong<sup>*</sup>, <strong>Wangyan Ying<sup>*</sup></strong>, Dongjie Wang, Yanjie Fu
                <br>
                <em>TIST</em>, 2025(
                <a href="https://github.com/NanxuGong/feature-selection-via-autoregreesive-generation">project page</a>
                /
                <a href="https://arxiv.org/abs/2404.17157">Paper</a>)
                <p></p>
                <p>
                  We propose a neuro-symbolic generative framework for feature selection, embedding feature subset knowledge into a continuous space using an encoder-decoder-evaluator model. 
                  The method emphasizes orthogonality by ensuring that selected features are both effective and minimally redundant. 
                  A gradient-based search strategy is employed to optimize feature subsets, enhancing generalizability and improving downstream model performance.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <img src='images/aaai.png' width=100%>
                </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">Evolutionary Large Language Model for Automated Feature Transformation</span>
                <br>
                Nanxu Gong, Chandan K Reddy, <strong>Wangyang Ying</strong>, Haifeng Chen, Yanjie Fu
                <br>
                <em>AAAI</em>, 2025 (
                <a href="https://github.com/NanxuGong/ELLM-FT/">project page</a>
                /
                <a href="https://arxiv.org/abs/2405.16203">Paper</a>)
                <p></p>
                <p>
                  We propose an evolutionary large language model (LLM) framework for automated feature transformation. 
                  By integrating LLMs with evolutionary algorithms, the method formulates feature transformation as a sequential generation task, 
                  leveraging few-shot prompting and multi-population evolutionary search to efficiently optimize feature transformations while balancing general and task-specific feature knowledge.
                </p>
              </td>
            </tr>

            
          </tbody></table>
          <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=uhfTMmoX5BN3kCx7ehcFg9vn3EZSF7OLLv0V00n5zxQ'></script>        
  </body>
</html>
